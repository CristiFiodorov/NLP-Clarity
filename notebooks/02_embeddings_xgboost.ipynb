{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import xgboost as xgb\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "MODELS_DIR = Path(\"models/embeddings_xgboost\")\n",
    "BASE_DRIVE_DIR = Path(\"/content/drive/MyDrive/NLP-Clarity\")\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    MODELS_DIR = BASE_DRIVE_DIR / \"models\" / \"embeddings_xgboost\"\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATA_DIR = BASE_DRIVE_DIR / \"data\"\n",
    "\n",
    "train_path = DATA_DIR / \"train.csv\"\n",
    "val_path = DATA_DIR / \"val.csv\"\n",
    "\n",
    "\n",
    "# - The 'test' split on HuggingFace (308 samples) IS the public leaderboard set.\n",
    "# - We treat this as our VALIDATION set ('df_val') to select the best model.\n",
    "# - We also save the train and val to disk, in case dataset from huggingface is updated (e.g., when evaluation phase will start).\n",
    "def load_qevasion_dataset():\n",
    "    if train_path.exists() and val_path.exists():\n",
    "        df_train = pd.read_csv(train_path)\n",
    "        df_val = pd.read_csv(val_path)\n",
    "        return df_train, df_val\n",
    "    else:\n",
    "        dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "        df_train = dataset[\"train\"].to_pandas()\n",
    "        df_val = dataset[\"test\"].to_pandas()\n",
    "        df_train.to_csv(train_path, index=False)\n",
    "        df_val.to_csv(val_path, index=False)\n",
    "        return df_train, df_val\n",
    "\n",
    "df_train, df_val = load_qevasion_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_for_class is the exact function used by the authors (they posted it on discord group)\n",
    "def f1_for_class(gold_annotations, predictions, target_class):\n",
    "    \"\"\"\n",
    "    Calculates Precision/Recall/F1 for only one class.\n",
    "\n",
    "    gold_annotations: list of lists (or sets) with labels per sample\n",
    "    predictions: list with one prediction per sample\n",
    "    target_class: the class for which we want the F1\n",
    "    \"\"\"\n",
    "    TP = FP = FN = 0\n",
    "\n",
    "    for gold, pred in zip(gold_annotations, predictions):\n",
    "        gold = set(gold)\n",
    "\n",
    "        if pred == target_class and target_class in gold:\n",
    "            TP += 1  # we correctly predicted target_class\n",
    "        elif pred == target_class and target_class not in gold:\n",
    "            FP += 1  # we predicted target_class but it was not in gold\n",
    "        elif target_class in gold and pred not in gold:\n",
    "            FN += 1  # the class was in gold but the sample is overall wrong\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"tp\": TP, \"fp\": FP, \"fn\": FN}\n",
    "\n",
    "\n",
    "def compute_macro_f1(gold_annotations, predictions):\n",
    "    \"\"\"\n",
    "    Compute Macro-F1 score (same as CodaBench leaderboard).\n",
    "\n",
    "    Args:\n",
    "        gold_annotations: list of lists - each inner list contains valid labels from annotators\n",
    "        predictions: list of strings - one prediction per sample\n",
    "\n",
    "    Returns:\n",
    "        float: Macro F1 score\n",
    "    \"\"\"\n",
    "    all_classes = set()\n",
    "    for gold in gold_annotations:\n",
    "        all_classes.update(gold)\n",
    "    classes = sorted(list(all_classes))\n",
    "\n",
    "    f1_scores = []\n",
    "    for cls in classes:\n",
    "        result = f1_for_class(gold_annotations, predictions, cls)\n",
    "        f1_scores.append(result[\"f1\"])\n",
    "\n",
    "    macro_f1 = float(np.mean(f1_scores))\n",
    "\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "embedder = SentenceTransformer(model_name)\n",
    "\n",
    "\n",
    "def extract_features(df):\n",
    "    \"\"\"\n",
    "    Features:\n",
    "        - Question embedding (768 dims)\n",
    "        - Answer embedding (768 dims)\n",
    "        - Cosine similarity (1 dim)\n",
    "    \n",
    "    All of the above is flattened.\n",
    "    \"\"\"\n",
    "    q_emb = embedder.encode(df['question'].tolist(), show_progress_bar=True)\n",
    "    a_emb = embedder.encode(df['interview_answer'].tolist(), show_progress_bar=True)\n",
    "    cos_sim = cosine_similarity(q_emb, a_emb).diagonal().reshape(-1, 1)\n",
    "    X = np.hstack([q_emb, a_emb, cos_sim])\n",
    "    return X\n",
    "\n",
    "\n",
    "X_train = extract_features(df_train)\n",
    "X_val = extract_features(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Experiment Tracking & Training\n",
    "\n",
    "**Use this cell to train models, track experiments, and save the best performing one.**\n",
    "\n",
    "This pipeline will:\n",
    "1. **Train XGBoost** with the specified configuration (hyperparameters).\n",
    "2. **Evaluate** performance on the validation set using the official Macro-F1 metric.\n",
    "3. **Log** every experiment to `experiment_log.json` (so you don't lose history).\n",
    "4. **Auto-Save Best Model**: If the current model beats the previous best F1 score, it automatically overwrites `best_model.pkl`.\n",
    "\n",
    "**How to use:**\n",
    "- Change the `config` dictionary at the bottom.\n",
    "- Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_LOG_PATH = MODELS_DIR / \"experiment_log.json\"\n",
    "BEST_SCORE_PATH = MODELS_DIR / \"best_score.json\"\n",
    "BEST_MODEL_PATH = MODELS_DIR / \"best_model.pkl\"\n",
    "\n",
    "\n",
    "def train_xgboost(config, X_train, y_train):\n",
    "    \"\"\"Train XGBoost with given config. Returns model and label_encoder.\"\"\"\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y_train)\n",
    "    \n",
    "    weights = compute_sample_weight('balanced', y_encoded)\n",
    "    \n",
    "    clf = xgb.XGBClassifier(\n",
    "        n_estimators=config[\"n_estimators\"],\n",
    "        max_depth=config[\"max_depth\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        objective='multi:softmax',\n",
    "        num_class=len(label_encoder.classes_),\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='mlogloss',\n",
    "        device='cuda'\n",
    "    )\n",
    "    clf.fit(X_train, y_encoded, sample_weight=weights)\n",
    "    \n",
    "    return clf, label_encoder\n",
    "\n",
    "\n",
    "def evaluate_model(clf, label_encoder, X_val, df_val):\n",
    "    \"\"\"Predict and compute macro F1 against annotator gold labels.\"\"\"\n",
    "    y_pred_encoded = clf.predict(X_val)\n",
    "    y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "    gold_annotations = df_val[['annotator1', 'annotator2', 'annotator3']].values.tolist()\n",
    "    macro_f1 = compute_macro_f1(gold_annotations, y_pred)\n",
    "    return macro_f1\n",
    "\n",
    "\n",
    "def log_experiment(config, macro_f1):\n",
    "    \"\"\"Append experiment to log file.\"\"\"\n",
    "    if RESULTS_LOG_PATH.exists():\n",
    "        with open(RESULTS_LOG_PATH, 'r') as f:\n",
    "            experiment_log = json.load(f)\n",
    "    else:\n",
    "        experiment_log = []\n",
    "    \n",
    "    experiment_log.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"config\": config,\n",
    "        \"macro_f1\": macro_f1\n",
    "    })\n",
    "    \n",
    "    with open(RESULTS_LOG_PATH, 'w') as f:\n",
    "        json.dump(experiment_log, f, indent=2)\n",
    "\n",
    "\n",
    "def save_if_best(clf, label_encoder, config, macro_f1):\n",
    "    \"\"\"Save model if it beats the current best. Returns True if saved.\"\"\"\n",
    "    if BEST_SCORE_PATH.exists():\n",
    "        with open(BEST_SCORE_PATH, 'r') as f:\n",
    "            best_f1 = json.load(f).get(\"macro_f1\", 0)\n",
    "    else:\n",
    "        best_f1 = 0\n",
    "    \n",
    "    if macro_f1 > best_f1:\n",
    "        with open(BEST_SCORE_PATH, 'w') as f:\n",
    "            json.dump({\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"config\": config,\n",
    "                \"macro_f1\": macro_f1\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        joblib.dump({\n",
    "            \"model\": clf,\n",
    "            \"label_encoder\": label_encoder,\n",
    "            \"config\": config\n",
    "        }, BEST_MODEL_PATH)\n",
    "\n",
    "        print(f\"New Best! F1: {macro_f1:.4f} (prev: {best_f1:.4f})\")\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def run_experiment(config):\n",
    "    clf, label_encoder = train_xgboost(config, X_train, df_train['evasion_label'])\n",
    "    macro_f1 = evaluate_model(clf, label_encoder, X_val, df_val)\n",
    "    log_experiment(config, macro_f1)\n",
    "    save_if_best(clf, label_encoder, config, macro_f1)\n",
    "    \n",
    "    return macro_f1\n",
    "\n",
    "# === RUN GRID SEARCH ===\n",
    "params_grid = {\n",
    "    \"n_estimators\": [200, 500, 1000],\n",
    "    \"max_depth\": [4, 6, 8, 10],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "keys = params_grid.keys()\n",
    "combinations = list(product(*params_grid.values()))\n",
    "results = []\n",
    "for i, values in enumerate(combinations):\n",
    "    config = {\n",
    "        \"model\": \"XGBoost\",\n",
    "        \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "        **dict(zip(keys, values))\n",
    "    }\n",
    "\n",
    "    print(f\"\\n[{i+1}/{len(combinations)}] {config}\")\n",
    "    f1 = run_experiment(config)\n",
    "    results.append((config, f1))\n",
    "\n",
    "sorted(results, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Run this cell ONLY to generate submission files for CodaBench.\n",
    "\n",
    "This pipeline will:\n",
    "1. Load your **best saved model** (`best_model.pkl`) from the models directory.\n",
    "2. Download the **\"test\" dataset** from HuggingFace.\n",
    "3. Generate predictions for both:\n",
    "   - **Task 2 (Evasion)**: Direct predictions from the model.\n",
    "   - **Task 1 (Clarity)**: Derived by mapping evasion labels to clarity categories.\n",
    "4. Save formatted `.zip` files ready for upload to CodaBench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVASION_TO_CLARITY = {\n",
    "    'Explicit': 'Clear Reply',\n",
    "    'Implicit': 'Ambivalent',\n",
    "    'Dodging': 'Ambivalent',\n",
    "    'General': 'Ambivalent',\n",
    "    'Deflection': 'Ambivalent',\n",
    "    'Partial/half-answer': 'Ambivalent',\n",
    "    'Declining to answer': 'Clear Non-Reply',\n",
    "    'Claims ignorance': 'Clear Non-Reply',\n",
    "    'Clarification': 'Clear Non-Reply',\n",
    "}\n",
    "\n",
    "SUBMISSIONS_DIR = MODELS_DIR / \"submissions\"\n",
    "\n",
    "\n",
    "def load_best_model():\n",
    "    \"\"\"Load best model, label encoder, and config from disk.\"\"\"\n",
    "    data = joblib.load(BEST_MODEL_PATH)\n",
    "    return data[\"model\"], data[\"label_encoder\"], data[\"config\"]\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    \"\"\"Download fresh test data from HuggingFace.\"\"\"\n",
    "    dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "    return dataset[\"test\"].to_pandas()\n",
    "\n",
    "\n",
    "def predict_evasion(clf, label_encoder, X):\n",
    "    \"\"\"Predict evasion labels.\"\"\"\n",
    "    y_encoded = clf.predict(X)\n",
    "    return label_encoder.inverse_transform(y_encoded)\n",
    "\n",
    "\n",
    "def evasion_to_clarity(y_evasion):\n",
    "    \"\"\"Map evasion labels to clarity labels.\"\"\"\n",
    "    return [EVASION_TO_CLARITY[e] for e in y_evasion]\n",
    "\n",
    "\n",
    "def save_submission(predictions, task_name):\n",
    "    \"\"\"Save predictions as a properly formatted zip for CodaBench.\"\"\"\n",
    "    SUBMISSIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    pred_path = SUBMISSIONS_DIR / f\"prediction_{task_name}\"\n",
    "    zip_path = SUBMISSIONS_DIR / f\"submission_{task_name}.zip\"\n",
    "    \n",
    "    with open(pred_path, 'w') as f:\n",
    "        f.write('\\n'.join(predictions))\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w') as zf:\n",
    "        zf.write(pred_path, \"prediction\")\n",
    "    \n",
    "    return zip_path\n",
    "\n",
    "\n",
    "def generate_submissions():\n",
    "    \"\"\"Full pipeline: load model → predict → save submissions.\"\"\"\n",
    "    clf, label_encoder, config = load_best_model()\n",
    "    \n",
    "    df_test = load_test_data()\n",
    "    X_test = extract_features(df_test)\n",
    "    \n",
    "    y_evasion = predict_evasion(clf, label_encoder, X_test)\n",
    "    y_clarity = evasion_to_clarity(y_evasion)\n",
    "    \n",
    "    zip_task2 = save_submission(y_evasion, \"task2\")\n",
    "    zip_task1 = save_submission(y_clarity, \"task1\")\n",
    "    \n",
    "    return {\n",
    "        \"task1_zip\": zip_task1,\n",
    "        \"task2_zip\": zip_task2,\n",
    "        \"evasion_dist\": Counter(y_evasion),\n",
    "        \"clarity_dist\": Counter(y_clarity),\n",
    "    }\n",
    "\n",
    "\n",
    "results = generate_submissions()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
