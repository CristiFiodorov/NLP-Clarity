{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VpJsCHtAJx9g"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re, sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    %pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    %pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    %pip install --no-deps unsloth\n",
    "else:\n",
    "    %pip install unsloth\n",
    "\n",
    "%pip install transformers==4.56.2\n",
    "%pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "\n",
    "MODELS_DIR = Path(\"models/llama31_8b_lora\")\n",
    "BASE_DRIVE_DIR = Path(\"/content/drive/MyDrive/NLP-Clarity\")\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    MODELS_DIR = BASE_DRIVE_DIR / \"models\" / \"llama31_8b_lora\"\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300,
     "referenced_widgets": [
      "6e3c281f112b4a86af7a3ef95933d221",
      "92395f250a154006923aaf9ea0a9c30b",
      "f84bfc5390054ec687c157c4d68199a6",
      "4228734651ca45e19fc7bda79817f9b3",
      "4613edbbec6846edb5b1677c25d542b6",
      "d032fe2ba5d647d99026fdade758c0cd",
      "e9971d220fe24552a1e9aa299765cfb9",
      "2be29a4553ad4dfea8a9bc620c81a3ae",
      "94cbb87829d1486899e2ff6325c2ecdf",
      "f878c2e00bc240c7b0333cce950080e1",
      "6b908368de51428585552dfef6a83088",
      "ac5eacaaee8346c080e54ea7a52648a4",
      "7981edf408d54d41bbeac42da7492c6b",
      "2b848e5a85bc42bc87945fd9ed5db038",
      "e88c33f37d6849e0b1a6b41254104cb9",
      "33843107b93647b28985bfc37ea781ca",
      "76e5410e286a4a5abd6c213a38aa38bb",
      "ae7e90a811f94e75997d6a9ed1be8596",
      "bb9f3379310d4b04be694996f3137b28",
      "75082ba15db445df907f5612976590ae",
      "89934c4f26834f15b9889ec36fee3b65",
      "e887160635cb4803b9f33845df615ec6",
      "1c7bc5fdb7dd4c39af8d4c2c504ec3ed",
      "843a27e619534ea8914f9d36386c364b",
      "8f5adc70fbf248f2811527f620553be5",
      "4ffb4b2f015046fb94c1115ed0397a20",
      "5a232ed040f94633a2a374031284c1f6",
      "2006be31c09349738e221295bb84939f",
      "07055fc12b0841aaa5317f8252b5d347",
      "1eb90e686e214122ae763b1b79ae321d",
      "7a935956348e47c68fbdf05ddf4752f3",
      "8653acb618ad4e76bbf1daa00ea71238",
      "e3c3bd9c4c124b0a8c88c83c1fc747d3",
      "1bd75ddaf57c4438a4e2c3070b9cef65",
      "a3a3ef6d6337403cabea8b23f7c3021b",
      "c2ea0a3f01f34ffa8c94ab9b5098e9da",
      "68ea1d7cb8274a639b3fb5326f4218c3",
      "39fef7b257614a0595f39355fa226b69",
      "d125995cc0934239a01ba01b78529f21",
      "634ae4c6cfe04673b1cdc9c9cac4cbf9",
      "d7f92e8332374313bee87ccd427446a4",
      "36799fbcd90d43128620ff98225a825d",
      "5310346dd579424fa676b8e8e64790e7",
      "0c1835f404db4846bb13b5da8d8f4447",
      "29c5b713f07043dda51820523e5c8ff3",
      "d7375f0f048841b29a20601c122666e8",
      "f433ced9bfcd4a57ba691d3c1caeed08",
      "da8ffc70820a48f5a12c6d4b5967015b",
      "1a6db9aea6a64ae3aaef51d6265b35b2",
      "331f516c7a76456d801bc2a2feb228aa",
      "9be9074028da42d39d044a78393a861f",
      "51cd9026b1664819a67712996ca97bd5",
      "ea55293415ca48a4be97c2e1e4769122",
      "8ea52b105a7e44978caca33c0e7e815b",
      "3662f1445ef34a50b462e601ed31bb69"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "0a47b925-663d-4543-9c61-994a6302f3c5"
   },
   "outputs": [],
   "source": [
    "# === MODEL CONFIG ===\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "3e2a4618-6aa0-4f1c-d3a0-0ec45eb33237"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATA_DIR = BASE_DRIVE_DIR / \"data\"\n",
    "\n",
    "train_path = DATA_DIR / \"train.csv\"\n",
    "val_path = DATA_DIR / \"val.csv\"\n",
    "\n",
    "\n",
    "# - The 'test' split on HuggingFace (308 samples) IS the public leaderboard set.\n",
    "# - We treat this as our VALIDATION set ('df_val') to select the best model.\n",
    "# - We also save the train and val to disk, in case dataset from huggingface is updated (e.g., when evaluation phase will start).\n",
    "def load_qevasion_dataset():\n",
    "    if train_path.exists() and val_path.exists():\n",
    "        df_train = pd.read_csv(train_path)\n",
    "        df_val = pd.read_csv(val_path)\n",
    "        return df_train, df_val\n",
    "    else:\n",
    "        dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "        df_train = dataset[\"train\"].to_pandas()\n",
    "        df_val = dataset[\"test\"].to_pandas()\n",
    "        df_train.to_csv(train_path, index=False)\n",
    "        df_val.to_csv(val_path, index=False)\n",
    "        return df_train, df_val\n",
    "\n",
    "df_train, df_val = load_qevasion_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "5e8825fb770b41529f2129113cebc4a9",
      "0a9dc233674e4096b7a988a5e4ebaf84",
      "374fa9beda4042e1bf9a9b13de6e6674",
      "e6533d3c91fd4359bc84ffd8e59af5a3",
      "f9b01aebcbdc48a585b7942b0ee60a2d",
      "d4b3770433bc41818372b7aed243fb31",
      "19bcefcc1d874840ae9a9ca983e474b6",
      "4011ce9370d74fad857ec8e1e99d314f",
      "41f5fed060ad4c8d87b24602b720ef04",
      "88fcd51819b5483c9ab22df7ef89ab64",
      "e6ffac074f1b476ba2ade11b37732af3",
      "98a6716e7438429ea322adb3e3264f91",
      "68c686291b50430faeef0de7840e2c4b",
      "953625aa1e824f8a8d203197b316b302",
      "f899a815142542219bde22ff792fb60c",
      "51ca174d26e94b5cb1e895aa3c770655",
      "f4519637bb43400a80ce83505101e8a5",
      "805676b197c94f5aa45956daa354640b",
      "ce9fcc5eff1f460d80b703a4ca32dad1",
      "3fef797403d14440afe599a3bf06b626",
      "4e7cb8e988114ed4b6fe09ff9f682dff",
      "a14bc1c2130842568a5fde6698731e5f",
      "85cc6f24cba54563acb5598f54fed7b9",
      "80a72037771e4da9be989eefabbc8e76",
      "ba68b274c50b44ec9e02642378d271a6",
      "f84d2fe4f1c24a34948755abf1f32b7f",
      "86511967834f4484a5ec4af387b7d7a9",
      "c97c40c2bf2a41a8ae1c75e0a9c8ebff",
      "484e507f14424f2b9173595b985f4101",
      "68a32b398e1c490393e01befdc260785",
      "04cc963133d242779572d2e847fa3d65",
      "94730f13e92a4c9aac35c2cfb21fc48c",
      "620c0de28ec74f71a021a2be96dccf3a",
      "6e1aff64771c402ab070f650562fa4c9",
      "0078f897f2174217a307d95d4f9bd775",
      "ecdeaab4f8c94d6dade63bb06857c969",
      "735b85f0a0e9411cac4d704a504fcfc1",
      "8e992e60416145a8b6eed744287ca0fb",
      "8c195b5809604905b5e404baa30e8449",
      "2247efac4283489bbd228330344388ab",
      "e93d063faf984cc4aa51462418d9b57e",
      "9d45b9a5de3e4cba9ac35ad2cb187f51",
      "e99423a1ed3f4f72886b39368468b7c1",
      "5f174718e5974a7cab024d113f662513"
     ]
    },
    "id": "LjY75GoYUCB8",
    "outputId": "80d6c3b9-28c2-4ebf-9c57-6a0b77ce82b1"
   },
   "outputs": [],
   "source": [
    "INSTRUCTION = \"\"\"You are an expert in political discourse analysis. Analyze the following question-answer pair from a political interview and classify the evasion strategy.\n",
    "\n",
    "Context: The question is a specific sub-question, but the answer is the full response. Focus only on the portion of the answer relevant to the sub-question.\n",
    "\n",
    "The taxonomy of responses consists of 3 main clarity levels, which are further divided into 9 specific evasion types:\n",
    "\n",
    "1. Clear Reply (Unambiguous)\n",
    "   - 'Explicit': The information requested is explicitly stated (in the requested form)\n",
    "\n",
    "2. Ambivalent Reply\n",
    "   - 'Implicit': The information requested is given, but without being explicitly stated (not in the expected form)\n",
    "   - 'General': The information provided is too general/lacks the requested specificity\n",
    "   - 'Partial/half-answer': Offers only a specific component of the requested information\n",
    "   - 'Dodging': Ignoring the question altogether\n",
    "   - 'Deflection': Starts on topic but shifts focus and makes a different point than asked\n",
    "\n",
    "3. Clear Non-Reply\n",
    "   - 'Declining to answer': Acknowledge the question but directly or indirectly refusing to answer at the moment\n",
    "   - 'Claims ignorance': The answerer claims/admits not to know the answer themselves\n",
    "   - 'Clarification': Does not provide the requested information and asks for clarification\n",
    "\n",
    "Task: Output ONLY the specific evasion type (e.g., 'Explicit', 'Deflection', etc.).\"\"\"\n",
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    questions = examples[\"question\"]\n",
    "    answers = examples[\"interview_answer\"]\n",
    "    labels = examples[\"evasion_label\"]\n",
    "    \n",
    "    texts = []\n",
    "    for question, answer, label in zip(questions, answers, labels):\n",
    "        input_text = f\"Question: {question}\\nAnswer: {answer}\"\n",
    "        text = alpaca_prompt.format(INSTRUCTION, input_text, label) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "def print_training_stats(trainer_stats):\n",
    "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "    used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "    lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "    print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "    print(\n",
    "        f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    "    )\n",
    "    print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "    print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "    print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "    print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_for_class is the exact function used by the authors (they posted it on discord group)\n",
    "def f1_for_class(gold_annotations, predictions, target_class):\n",
    "    \"\"\"\n",
    "    Calculates Precision/Recall/F1 for only one class.\n",
    "\n",
    "    gold_annotations: list of lists (or sets) with labels per sample\n",
    "    predictions: list with one prediction per sample\n",
    "    target_class: the class for which we want the F1\n",
    "    \"\"\"\n",
    "    TP = FP = FN = 0\n",
    "\n",
    "    for gold, pred in zip(gold_annotations, predictions):\n",
    "        gold = set(gold)\n",
    "\n",
    "        if pred == target_class and target_class in gold:\n",
    "            TP += 1  # we correctly predicted target_class\n",
    "        elif pred == target_class and target_class not in gold:\n",
    "            FP += 1  # we predicted target_class but it was not in gold\n",
    "        elif target_class in gold and pred not in gold:\n",
    "            FN += 1  # the class was in gold but the sample is overall wrong\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"tp\": TP, \"fp\": FP, \"fn\": FN}\n",
    "\n",
    "\n",
    "def compute_macro_f1(gold_annotations, predictions):\n",
    "    \"\"\"\n",
    "    Compute Macro-F1 score (same as CodaBench leaderboard).\n",
    "\n",
    "    Args:\n",
    "        gold_annotations: list of lists - each inner list contains valid labels from annotators\n",
    "        predictions: list of strings - one prediction per sample\n",
    "\n",
    "    Returns:\n",
    "        float: Macro F1 score\n",
    "    \"\"\"\n",
    "    all_classes = set()\n",
    "    for gold in gold_annotations:\n",
    "        all_classes.update(gold)\n",
    "    classes = sorted(list(all_classes))\n",
    "\n",
    "    f1_scores = []\n",
    "    for cls in classes:\n",
    "        result = f1_for_class(gold_annotations, predictions, cls)\n",
    "        f1_scores.append(result[\"f1\"])\n",
    "\n",
    "    macro_f1 = float(np.mean(f1_scores))\n",
    "\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_evasion_labels(model, tokenizer, df):\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    predictions = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Predicting\"):\n",
    "        input_text = f\"Question: {row['question']}\\nAnswer: {row['interview_answer']}\"\n",
    "        prompt = alpaca_prompt.format(INSTRUCTION, input_text, \"\")\n",
    "        \n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens=20, use_cache=True)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        if \"### Response:\" in response:\n",
    "            pred_label = response.split(\"### Response:\")[-1].strip()\n",
    "        else:\n",
    "            pred_label = response.strip()\n",
    "        \n",
    "        pred_label = pred_label.split(\"\\n\")[0].strip()\n",
    "        predictions.append(pred_label)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def evaluate_model(model, tokenizer, df_val):\n",
    "    predictions = predict_evasion_labels(model, tokenizer, df_val)\n",
    "    gold_annotations = df_val[['annotator1', 'annotator2', 'annotator3']].values.tolist()\n",
    "    macro_f1 = compute_macro_f1(gold_annotations, predictions)\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "RESULTS_LOG_PATH = MODELS_DIR / \"experiment_log.json\"\n",
    "BEST_SCORE_PATH = MODELS_DIR / \"best_score.json\"\n",
    "\n",
    "\n",
    "def log_experiment(config, macro_f1):\n",
    "    \"\"\"Append experiment to log file.\"\"\"\n",
    "    if RESULTS_LOG_PATH.exists():\n",
    "        with open(RESULTS_LOG_PATH, 'r') as f:\n",
    "            experiment_log = json.load(f)\n",
    "    else:\n",
    "        experiment_log = []\n",
    "    \n",
    "    experiment_log.append({\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"config\": config,\n",
    "        \"train_data\": train_path,\n",
    "        \"macro_f1\": macro_f1\n",
    "    })\n",
    "    \n",
    "    with open(RESULTS_LOG_PATH, 'w') as f:\n",
    "        json.dump(experiment_log, f, indent=2)\n",
    "\n",
    "\n",
    "def save_if_best(model, tokenizer, config, macro_f1):\n",
    "    \"\"\"Save LoRA model if it beats current best. Returns True if saved.\"\"\"\n",
    "    if BEST_SCORE_PATH.exists():\n",
    "        with open(BEST_SCORE_PATH, 'r') as f:\n",
    "            best_f1 = json.load(f).get(\"macro_f1\", 0)\n",
    "    else:\n",
    "        best_f1 = 0\n",
    "    \n",
    "    if macro_f1 > best_f1:\n",
    "        with open(BEST_SCORE_PATH, 'w') as f:\n",
    "            json.dump({\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"config\": config,\n",
    "                \"macro_f1\": macro_f1\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        model.save_pretrained(MODELS_DIR / \"best_lora\")\n",
    "        tokenizer.save_pretrained(MODELS_DIR / \"best_lora\")\n",
    "        \n",
    "        print(f\"New Best! F1: {macro_f1:.4f} (prev: {best_f1:.4f})\")\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use this cell to train models, track experiments, and save the best performing one.**\n",
    "\n",
    "This pipeline will:\n",
    "1. **Train Llama 3.1 8B** with LoRA adapters using the specified configuration.\n",
    "2. **Evaluate** performance on the validation set using the official Macro-F1 metric.\n",
    "3. **Log** every experiment to `experiment_log.json` (so you don't lose history).\n",
    "4. **Auto-Save Best Model**: If the current model beats the previous best F1 score, it automatically saves LoRA adapters to `best_lora/`.\n",
    "\n",
    "**How to use:**\n",
    "- Change the `config` dictionary below.\n",
    "- Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config):\n",
    "    trainer = SFTTrainer(\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        train_dataset = train_dataset,\n",
    "        dataset_text_field = \"text\",\n",
    "        max_seq_length = max_seq_length,\n",
    "        packing = False, # Can make training 5x faster for short sequences.\n",
    "        args = SFTConfig(\n",
    "            per_device_train_batch_size = config.get(\"batch_size\", 2),\n",
    "            gradient_accumulation_steps = config.get(\"grad_accum\", 4),\n",
    "            warmup_steps = config.get(\"warmup_steps\", 5),\n",
    "            num_train_epochs = config[\"epochs\"],\n",
    "            learning_rate = config[\"learning_rate\"],\n",
    "            logging_steps = 1,\n",
    "            optim = \"adamw_8bit\",\n",
    "            weight_decay = config.get(\"weight_decay\", 0.001),\n",
    "            lr_scheduler_type = \"linear\",\n",
    "            seed = 3407,\n",
    "            output_dir = str(MODELS_DIR / \"checkpoints\"),\n",
    "            report_to = \"none\", # Use TrackIO/WandB etc\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    trainer_stats = trainer.train()\n",
    "    print_training_stats(trainer_stats)\n",
    "\n",
    "    macro_f1 = evaluate_model(model, tokenizer, df_val)\n",
    "\n",
    "    log_experiment(config, macro_f1)\n",
    "    save_if_best(model, tokenizer, config, macro_f1)\n",
    "    \n",
    "    return macro_f1, trainer_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\": \"Llama-3.1-8B\",\n",
    "    \"adapter\": \"LoRA\",\n",
    "    \"r\": 16,\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"instruction\": INSTRUCTION, \n",
    "    \"batch_size\": 16,\n",
    "    \"grad_accum\": 2,\n",
    "}\n",
    "\n",
    "macro_f1, stats = run_experiment(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell ONLY to generate submission files for CodaBench.\n",
    "\n",
    "This pipeline will:\n",
    "1. Load your **best saved LoRA model** (`best_lora/`) from the models directory.\n",
    "2. Download the **\"test\" dataset** from HuggingFace.\n",
    "3. Generate predictions for both:\n",
    "   - **Task 2 (Evasion)**: Direct predictions from the model (9 labels).\n",
    "   - **Task 1 (Clarity)**: Derived by mapping evasion labels to clarity categories (3 labels).\n",
    "4. Save formatted `.zip` files ready for upload to CodaBench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVASION_TO_CLARITY = {\n",
    "    'Explicit': 'Clear Reply',\n",
    "    'Implicit': 'Ambivalent',\n",
    "    'Dodging': 'Ambivalent',\n",
    "    'General': 'Ambivalent',\n",
    "    'Deflection': 'Ambivalent',\n",
    "    'Partial/half-answer': 'Ambivalent',\n",
    "    'Declining to answer': 'Clear Non-Reply',\n",
    "    'Claims ignorance': 'Clear Non-Reply',\n",
    "    'Clarification': 'Clear Non-Reply',\n",
    "}\n",
    "\n",
    "SUBMISSIONS_DIR = MODELS_DIR / \"submissions\"\n",
    "\n",
    "\n",
    "def load_best_model():\n",
    "    \"\"\"Load best LoRA model from disk.\"\"\"\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = str(MODELS_DIR / \"best_lora\"),\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    \"\"\"Download fresh test data from HuggingFace.\"\"\"\n",
    "    dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "    return dataset[\"test\"].to_pandas()\n",
    "\n",
    "\n",
    "def evasion_to_clarity(y_evasion):\n",
    "    \"\"\"Map evasion labels to clarity labels.\"\"\"\n",
    "    return [EVASION_TO_CLARITY[e] for e in y_evasion]\n",
    "\n",
    "\n",
    "def save_submission(predictions, task_name):\n",
    "    \"\"\"Save predictions as a properly formatted zip for CodaBench.\"\"\"\n",
    "    SUBMISSIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    pred_path = SUBMISSIONS_DIR / f\"prediction_{task_name}\"\n",
    "    zip_path = SUBMISSIONS_DIR / f\"submission_{task_name}.zip\"\n",
    "    \n",
    "    with open(pred_path, 'w') as f:\n",
    "        f.write('\\n'.join(predictions))\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w') as zf:\n",
    "        zf.write(pred_path, \"prediction\")\n",
    "    \n",
    "    return zip_path\n",
    "\n",
    "\n",
    "def generate_submissions():\n",
    "    \"\"\"Full pipeline: load model → predict → save submissions.\"\"\"\n",
    "    best_model, best_tokenizer = load_best_model()\n",
    "    \n",
    "    df_test = load_test_data()\n",
    "    \n",
    "    y_evasion = predict_evasion_labels(best_model, best_tokenizer, df_test)\n",
    "    y_clarity = evasion_to_clarity(y_evasion)\n",
    "    \n",
    "    zip_task2 = save_submission(y_evasion, \"task2\")\n",
    "    zip_task1 = save_submission(y_clarity, \"task1\")\n",
    "    \n",
    "    return {\n",
    "        \"task1_zip\": zip_task1,\n",
    "        \"task2_zip\": zip_task2,\n",
    "        \"evasion_dist\": Counter(y_evasion),\n",
    "        \"clarity_dist\": Counter(y_clarity),\n",
    "    }\n",
    "\n",
    "results = generate_submissions()\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
